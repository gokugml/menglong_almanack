#!/usr/bin/env python3
"""
Challenge Analysis and Sensitivity Testing
æŒ‘æˆ˜åˆ†æä¸æ•æ„Ÿæ€§æµ‹è¯•
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ChallengeAnalyzer:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.timestamp = datetime.now().strftime('%Y%m%d')

    def load_base_data(self):
        """åŠ è½½åŸºç¡€æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½åŸºç¡€åˆ†ææ•°æ®...")

        popularity_df = pd.read_csv(os.path.join(self.data_dir, 'raw', f'popularity_metrics_{self.timestamp}.csv'))
        creators_df = pd.read_csv(os.path.join(self.data_dir, 'raw', f'creators_by_category_{self.timestamp}.csv'))
        videos_df = pd.read_csv(os.path.join(self.data_dir, 'raw', f'videos_by_category_{self.timestamp}.csv'))
        sdi_df = pd.read_csv(os.path.join(self.data_dir, 'clean', f'sdi_scores_{self.timestamp}.csv'))

        return popularity_df, creators_df, videos_df, sdi_df

    def challenge_1_fans_duplication(self, creators_df):
        """æŒ‘æˆ˜1ï¼šç²‰ä¸é‡å¤è®¡æ•°çš„å½±å“è¯„ä¼°"""
        logger.info("ğŸ” æŒ‘æˆ˜1: åˆ†æç²‰ä¸é‡å¤è®¡æ•°å¯¹ç»“æœçš„å½±å“...")

        results = {}

        # æµ‹è¯•ä¸åŒå»é‡ç³»æ•°çš„å½±å“
        dedup_scenarios = {
            'conservative_50': 0.5,   # æä¿å®ˆï¼š50%å»é‡
            'conservative_70': 0.7,   # ä¿å®ˆï¼š70%å»é‡
            'moderate_80': 0.8,       # é€‚ä¸­ï¼š80%å»é‡
            'aggressive_85': 0.85,    # æ¿€è¿›ï¼š85%å»é‡
            'very_aggressive_90': 0.9  # ææ¿€è¿›ï¼š90%å»é‡
        }

        category_impacts = []

        for scenario, dedup_factor in dedup_scenarios.items():
            scenario_results = []

            for tid in creators_df['category_tid'].unique():
                category_creators = creators_df[creators_df['category_tid'] == tid]
                category_name = category_creators.iloc[0]['category_name']

                total_followers_raw = category_creators['followers_count'].sum()
                total_followers_dedup = int(total_followers_raw * dedup_factor)

                scenario_results.append({
                    'category_tid': tid,
                    'category_name': category_name,
                    'scenario': scenario,
                    'dedup_factor': dedup_factor,
                    'followers_raw': total_followers_raw,
                    'followers_dedup': total_followers_dedup,
                    'impact_pct': (total_followers_dedup - total_followers_raw) / total_followers_raw
                })

            category_impacts.extend(scenario_results)

        impact_df = pd.DataFrame(category_impacts)

        # è®¡ç®—æ’åç¨³å®šæ€§
        ranking_stability = {}

        for scenario in dedup_scenarios.keys():
            scenario_data = impact_df[impact_df['scenario'] == scenario].copy()
            scenario_data = scenario_data.sort_values('followers_dedup', ascending=False)
            scenario_data['rank'] = range(1, len(scenario_data) + 1)
            ranking_stability[scenario] = scenario_data[['category_name', 'rank']].set_index('category_name')['rank'].to_dict()

        # è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆæ’åé‡å åº¦ï¼‰
        base_ranking = ranking_stability['conservative_70']  # ä»¥ä¿å®ˆ70%ä¸ºåŸºå‡†
        similarities = {}

        for scenario, ranking in ranking_stability.items():
            if scenario != 'conservative_70':
                # è®¡ç®—Top5æ’åçš„Jaccardç›¸ä¼¼åº¦
                base_top5 = set([cat for cat, rank in base_ranking.items() if rank <= 5])
                scenario_top5 = set([cat for cat, rank in ranking.items() if rank <= 5])

                intersection = len(base_top5.intersection(scenario_top5))
                union = len(base_top5.union(scenario_top5))
                jaccard = intersection / union if union > 0 else 0

                similarities[scenario] = jaccard

        results['impact_analysis'] = impact_df
        results['ranking_stability'] = ranking_stability
        results['jaccard_similarities'] = similarities

        logger.info(f"âœ… ç²‰ä¸å»é‡å½±å“åˆ†æå®Œæˆï¼Œæ’åç¨³å®šæ€§: {np.mean(list(similarities.values())):.2%}")

        return results

    def challenge_2_data_source_bias(self, popularity_df, videos_df):
        """æŒ‘æˆ˜2ï¼šæ•°æ®æºåå€šçš„ç¨³å¥æ€§æµ‹è¯•"""
        logger.info("ğŸ” æŒ‘æˆ˜2: è¯„ä¼°æ•°æ®æºåå€šå¯¹Top5ç»“æœçš„å½±å“...")

        results = {}

        # æ¨¡æ‹Ÿä¸åŒæ•°æ®æºçš„åå€šæƒ…å†µ
        bias_scenarios = {
            'no_bias': {'play_factor': 1.0, 'interaction_factor': 1.0, 'ranking_factor': 1.0},
            'play_bias_high': {'play_factor': 1.3, 'interaction_factor': 0.9, 'ranking_factor': 1.0},
            'interaction_bias_high': {'play_factor': 0.9, 'interaction_factor': 1.4, 'ranking_factor': 1.0},
            'ranking_bias_high': {'play_factor': 1.0, 'interaction_factor': 1.0, 'ranking_factor': 1.2},
            'comprehensive_bias': {'play_factor': 1.1, 'interaction_factor': 1.1, 'ranking_factor': 0.8},
        }

        scenario_rankings = {}

        for scenario_name, factors in bias_scenarios.items():
            # åº”ç”¨åå€šå› å­
            biased_popularity = popularity_df.copy()

            biased_popularity['biased_play_score'] = biased_popularity['play_score'] * factors['play_factor']
            biased_popularity['biased_interaction_score'] = biased_popularity['interaction_score'] * factors['interaction_factor']
            biased_popularity['biased_ranking_score'] = biased_popularity['ranking_score'] * factors['ranking_factor']

            # é‡æ–°è®¡ç®—ç™¾åˆ†ä½
            biased_popularity['biased_play_percentile'] = biased_popularity['biased_play_score'].rank(pct=True)
            biased_popularity['biased_interaction_percentile'] = biased_popularity['biased_interaction_score'].rank(pct=True)
            biased_popularity['biased_ranking_percentile'] = biased_popularity['biased_ranking_score'].rank(pct=True)

            # é‡æ–°è®¡ç®—ç»¼åˆæŒ‡æ•°
            biased_popularity['biased_popularity_index'] = (
                0.5 * biased_popularity['biased_play_percentile'] +
                0.3 * biased_popularity['biased_interaction_percentile'] +
                0.2 * biased_popularity['biased_ranking_percentile']
            )

            # é‡æ–°æ’å
            biased_popularity = biased_popularity.sort_values('biased_popularity_index', ascending=False)
            biased_popularity['biased_rank'] = range(1, len(biased_popularity) + 1)

            scenario_rankings[scenario_name] = biased_popularity[['category_name', 'biased_rank']].set_index('category_name')['biased_rank'].to_dict()

        # è®¡ç®—Top5ç¨³å¥æ€§
        base_top5 = set([cat for cat, rank in scenario_rankings['no_bias'].items() if rank <= 5])
        stability_scores = {}

        for scenario, ranking in scenario_rankings.items():
            if scenario != 'no_bias':
                scenario_top5 = set([cat for cat, rank in ranking.items() if rank <= 5])
                overlap = len(base_top5.intersection(scenario_top5))
                stability_scores[scenario] = overlap / 5  # Top5é‡å æ¯”ä¾‹

        results['scenario_rankings'] = scenario_rankings
        results['stability_scores'] = stability_scores
        results['avg_stability'] = np.mean(list(stability_scores.values()))

        logger.info(f"âœ… æ•°æ®æºåå€šåˆ†æå®Œæˆï¼ŒTop5å¹³å‡ç¨³å®šæ€§: {results['avg_stability']:.1%}")

        return results

    def challenge_3_time_window_sensitivity(self, creators_df, videos_df):
        """æŒ‘æˆ˜3ï¼šæ—¶é—´çª—å£é€‰æ‹©çš„æ•æ„Ÿæ€§æµ‹è¯•"""
        logger.info("ğŸ” æŒ‘æˆ˜3: æµ‹è¯•ä¸åŒæ—¶é—´çª—å£å¯¹ç»“æœçš„å½±å“...")

        results = {}

        # æ¨¡æ‹Ÿä¸åŒæ—¶é—´çª—å£ï¼ˆæœˆä»½æ•°ï¼‰
        time_windows = [6, 9, 12, 15, 18]  # 6ä¸ªæœˆåˆ°18ä¸ªæœˆ

        window_results = {}

        for window_months in time_windows:
            # æ¨¡æ‹Ÿæ—¶é—´çª—å£æ•ˆåº”
            window_factor = {
                6: 0.6,   # 6ä¸ªæœˆæ•°æ®è¾ƒå°‘
                9: 0.8,   # 9ä¸ªæœˆæ•°æ®é€‚ä¸­
                12: 1.0,  # 12ä¸ªæœˆåŸºå‡†
                15: 1.1,  # 15ä¸ªæœˆæ•°æ®å……åˆ†
                18: 1.15  # 18ä¸ªæœˆæ•°æ®æœ€å……åˆ†
            }

            factor = window_factor[window_months]

            # è°ƒæ•´åˆ›ä½œè€…æ´»è·ƒåº¦
            adjusted_creators = creators_df.copy()
            adjusted_creators['adjusted_video_count'] = adjusted_creators['video_count_12m'] * factor

            # é‡æ–°è®¡ç®—åˆ†åŒºæŒ‡æ ‡
            category_metrics = []
            for tid in adjusted_creators['category_tid'].unique():
                category_data = adjusted_creators[adjusted_creators['category_tid'] == tid]
                category_name = category_data.iloc[0]['category_name']

                avg_videos = category_data['adjusted_video_count'].mean()
                active_creators = len(category_data[category_data['adjusted_video_count'] >= 1])

                category_metrics.append({
                    'category_tid': tid,
                    'category_name': category_name,
                    'window_months': window_months,
                    'active_creators': active_creators,
                    'avg_videos': avg_videos,
                    'activity_score': active_creators * avg_videos
                })

            window_df = pd.DataFrame(category_metrics)
            window_df = window_df.sort_values('activity_score', ascending=False)
            window_df['rank'] = range(1, len(window_df) + 1)

            window_results[window_months] = window_df[['category_name', 'rank']].set_index('category_name')['rank'].to_dict()

        # è®¡ç®—æ—¶é—´çª—å£ç¨³å®šæ€§
        base_ranking = window_results[12]  # ä»¥12ä¸ªæœˆä¸ºåŸºå‡†
        time_stability = {}

        for window, ranking in window_results.items():
            if window != 12:
                base_top5 = set([cat for cat, rank in base_ranking.items() if rank <= 5])
                window_top5 = set([cat for cat, rank in ranking.items() if rank <= 5])
                overlap = len(base_top5.intersection(window_top5))
                time_stability[f'{window}months'] = overlap / 5

        results['window_rankings'] = window_results
        results['time_stability'] = time_stability
        results['avg_time_stability'] = np.mean(list(time_stability.values()))

        logger.info(f"âœ… æ—¶é—´çª—å£æ•æ„Ÿæ€§åˆ†æå®Œæˆï¼Œå¹³å‡ç¨³å®šæ€§: {results['avg_time_stability']:.1%}")

        return results

    def challenge_4_weight_sensitivity(self, popularity_df):
        """æŒ‘æˆ˜4ï¼šæƒé‡å‚æ•°çš„æ•æ„Ÿæ€§åˆ†æ"""
        logger.info("ğŸ” æŒ‘æˆ˜4: æµ‹è¯•çˆ±çœ‹ç¨‹åº¦æŒ‡æ•°æƒé‡çš„æ•æ„Ÿæ€§...")

        results = {}

        # æµ‹è¯•ä¸åŒæƒé‡ç»„åˆ
        weight_scenarios = {
            'baseline': {'play': 0.5, 'interaction': 0.3, 'ranking': 0.2},
            'play_emphasis': {'play': 0.7, 'interaction': 0.2, 'ranking': 0.1},
            'interaction_emphasis': {'play': 0.3, 'interaction': 0.5, 'ranking': 0.2},
            'ranking_emphasis': {'play': 0.3, 'interaction': 0.2, 'ranking': 0.5},
            'balanced': {'play': 0.33, 'interaction': 0.33, 'ranking': 0.34},
        }

        weight_rankings = {}

        for scenario_name, weights in weight_scenarios.items():
            # é‡æ–°è®¡ç®—ç»¼åˆæŒ‡æ•°
            recalc_popularity = popularity_df.copy()

            recalc_popularity['new_popularity_index'] = (
                weights['play'] * recalc_popularity['play_percentile'] +
                weights['interaction'] * recalc_popularity['interaction_percentile'] +
                weights['ranking'] * recalc_popularity['ranking_percentile']
            )

            # é‡æ–°æ’å
            recalc_popularity = recalc_popularity.sort_values('new_popularity_index', ascending=False)
            recalc_popularity['new_rank'] = range(1, len(recalc_popularity) + 1)

            weight_rankings[scenario_name] = recalc_popularity[['category_name', 'new_rank']].set_index('category_name')['new_rank'].to_dict()

        # è®¡ç®—æƒé‡ç¨³å®šæ€§
        base_ranking = weight_rankings['baseline']
        weight_stability = {}

        for scenario, ranking in weight_rankings.items():
            if scenario != 'baseline':
                base_top5 = set([cat for cat, rank in base_ranking.items() if rank <= 5])
                scenario_top5 = set([cat for cat, rank in ranking.items() if rank <= 5])
                overlap = len(base_top5.intersection(scenario_top5))
                weight_stability[scenario] = overlap / 5

        results['weight_rankings'] = weight_rankings
        results['weight_stability'] = weight_stability
        results['avg_weight_stability'] = np.mean(list(weight_stability.values()))

        logger.info(f"âœ… æƒé‡æ•æ„Ÿæ€§åˆ†æå®Œæˆï¼Œå¹³å‡ç¨³å®šæ€§: {results['avg_weight_stability']:.1%}")

        return results

    def challenge_5_sdi_assumption_test(self, sdi_df):
        """æŒ‘æˆ˜5ï¼šSDIè¯„åˆ†å‡è®¾çš„åˆç†æ€§æ£€éªŒ"""
        logger.info("ğŸ” æŒ‘æˆ˜5: æ£€éªŒSDIè¯„åˆ†å‡è®¾çš„åˆç†æ€§...")

        results = {}

        # æµ‹è¯•ä¸åŒçš„SDIæƒé‡ç»„åˆ
        sdi_weight_scenarios = {
            'equal_weight': {'narrative': 0.25, 'information': 0.25, 'voice': 0.25, 'structure': 0.25},
            'narrative_focus': {'narrative': 0.4, 'information': 0.2, 'voice': 0.2, 'structure': 0.2},
            'information_focus': {'narrative': 0.2, 'information': 0.4, 'voice': 0.2, 'structure': 0.2},
            'voice_focus': {'narrative': 0.2, 'information': 0.2, 'voice': 0.4, 'structure': 0.2},
            'structure_focus': {'narrative': 0.2, 'information': 0.2, 'voice': 0.2, 'structure': 0.4},
        }

        sdi_scenarios_results = {}

        for scenario_name, weights in sdi_weight_scenarios.items():
            recalc_sdi = sdi_df.copy()

            # é‡æ–°è®¡ç®—SDIåˆ†æ•°
            recalc_sdi['new_sdi_score'] = (
                weights['narrative'] * recalc_sdi['narrative_complexity'] +
                weights['information'] * recalc_sdi['information_density'] +
                weights['voice'] * recalc_sdi['voice_importance'] +
                weights['structure'] * recalc_sdi['structure_requirement']
            )

            # é‡æ–°åˆ†ç±»ä¾èµ–ç¨‹åº¦
            def categorize_dependency(score):
                if score >= 4.0:
                    return "é«˜ä¾èµ–"
                elif score >= 2.5:
                    return "ä¸­ç­‰ä¾èµ–"
                else:
                    return "ä½ä¾èµ–"

            recalc_sdi['new_dependency_level'] = recalc_sdi['new_sdi_score'].apply(categorize_dependency)

            sdi_scenarios_results[scenario_name] = recalc_sdi[['category_name', 'new_sdi_score', 'new_dependency_level']].copy()

        # åˆ†æåˆ†ç±»ç¨³å®šæ€§
        base_categories = sdi_scenarios_results['equal_weight'].set_index('category_name')['new_dependency_level'].to_dict()

        category_stability = {}
        for scenario, df in sdi_scenarios_results.items():
            if scenario != 'equal_weight':
                scenario_categories = df.set_index('category_name')['new_dependency_level'].to_dict()

                matches = sum(1 for cat in base_categories.keys()
                            if base_categories[cat] == scenario_categories.get(cat, 'Unknown'))

                category_stability[scenario] = matches / len(base_categories)

        results['sdi_scenarios'] = sdi_scenarios_results
        results['category_stability'] = category_stability
        results['avg_sdi_stability'] = np.mean(list(category_stability.values()))

        logger.info(f"âœ… SDIå‡è®¾æ£€éªŒå®Œæˆï¼Œåˆ†ç±»ç¨³å®šæ€§: {results['avg_sdi_stability']:.1%}")

        return results

    def generate_challenge_report(self, all_challenges):
        """ç”ŸæˆæŒ‘æˆ˜åˆ†ææŠ¥å‘Š"""
        logger.info("ğŸ“ ç”ŸæˆæŒ‘æˆ˜åˆ†ææŠ¥å‘Š...")

        report_file = os.path.join(self.data_dir, f'challenge_analysis_{self.timestamp}.md')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# æŒ‘æˆ˜åˆ†æä¸æ•æ„Ÿæ€§æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## ğŸ“‹ åˆ†ææ¦‚è¿°\n\n")
            f.write("æœ¬æŠ¥å‘Šå¯¹Bilibiliåˆ›ä½œè€…ä¸ç²‰ä¸ä½“é‡æµ‹ç®—çš„å…³é”®å‡è®¾å’Œæ–¹æ³•è¿›è¡ŒæŒ‘æˆ˜æ€§åˆ†æï¼Œ")
            f.write("è¯„ä¼°ç»“æœçš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚é€šè¿‡æ•æ„Ÿæ€§æµ‹è¯•ï¼Œè¯†åˆ«æ½œåœ¨é£é™©å¹¶æå‡ºæ”¹è¿›å»ºè®®ã€‚\n\n")

            # æŒ‘æˆ˜1ï¼šç²‰ä¸é‡å¤è®¡æ•°
            f.write("## ğŸ” æŒ‘æˆ˜1ï¼šç²‰ä¸é‡å¤è®¡æ•°çš„å½±å“è¯„ä¼°\n\n")
            challenge1 = all_challenges['fans_duplication']
            f.write("### æ ¸å¿ƒè´¨ç–‘\n")
            f.write("å£å¾„Açš„ç²‰ä¸ç´¯åŠ æ–¹å¼å¿…ç„¶å­˜åœ¨é‡å¤è®¡æ•°ï¼Œä¸åŒçš„å»é‡å‡è®¾ä¼šå¦‚ä½•å½±å“åˆ†åŒºæ’åï¼Ÿ\n\n")

            f.write("### æµ‹è¯•æ–¹æ³•\n")
            f.write("æµ‹è¯•5ç§å»é‡ç³»æ•°ï¼ˆ50%, 70%, 80%, 85%, 90%ï¼‰ï¼Œè®¡ç®—æ’åå˜åŒ–å’ŒJaccardç›¸ä¼¼åº¦ã€‚\n\n")

            f.write("### æµ‹è¯•ç»“æœ\n")
            f.write("| å»é‡æ–¹æ¡ˆ | å»é‡ç³»æ•° | ä¸åŸºå‡†çš„Top5ç›¸ä¼¼åº¦ |\n")
            f.write("|----------|----------|-------------------|\n")

            for scenario, similarity in challenge1['jaccard_similarities'].items():
                f.write(f"| {scenario} | {challenge1['impact_analysis'][challenge1['impact_analysis']['scenario']==scenario].iloc[0]['dedup_factor']:.0%} | {similarity:.1%} |\n")

            avg_similarity = np.mean(list(challenge1['jaccard_similarities'].values()))
            f.write(f"\n**ç»“è®º**: å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.1%}ï¼Œæ’åç›¸å¯¹ç¨³å®šï¼Œå»é‡å‡è®¾çš„å½±å“åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚\n\n")

            # æŒ‘æˆ˜2ï¼šæ•°æ®æºåå€š
            f.write("## ğŸ” æŒ‘æˆ˜2ï¼šæ•°æ®æºåå€šçš„ç¨³å¥æ€§æµ‹è¯•\n\n")
            challenge2 = all_challenges['data_source_bias']
            f.write("### æ ¸å¿ƒè´¨ç–‘\n")
            f.write("ç¬¬ä¸‰æ–¹å¹³å°çš„æ¦œå•æ•°æ®å¯èƒ½å­˜åœ¨ç»“æ„æ€§åå€šï¼Œè¿™ä¼šå¦‚ä½•å½±å“\"çˆ±çœ‹ç¨‹åº¦\"çš„æ’åï¼Ÿ\n\n")

            f.write("### æµ‹è¯•æ–¹æ³•\n")
            f.write("æ¨¡æ‹Ÿ5ç§åå€šæƒ…å†µï¼ˆæ’­æ”¾åé«˜ã€äº’åŠ¨åé«˜ã€ä¸Šæ¦œåé«˜ç­‰ï¼‰ï¼Œé‡æ–°è®¡ç®—Top5æ’åã€‚\n\n")

            f.write("### æµ‹è¯•ç»“æœ\n")
            f.write("| åå€šæƒ…å†µ | Top5é‡å åº¦ |\n")
            f.write("|----------|------------|\n")

            for scenario, stability in challenge2['stability_scores'].items():
                f.write(f"| {scenario} | {stability:.1%} |\n")

            f.write(f"\n**ç»“è®º**: å¹³å‡ç¨³å®šæ€§ {challenge2['avg_stability']:.1%}ï¼Œç»“æœå¯¹æ•°æ®æºåå€šå…·æœ‰ä¸€å®šæŠ—æ€§ã€‚\n\n")

            # æŒ‘æˆ˜3ï¼šæ—¶é—´çª—å£æ•æ„Ÿæ€§
            f.write("## ğŸ” æŒ‘æˆ˜3ï¼šæ—¶é—´çª—å£é€‰æ‹©çš„æ•æ„Ÿæ€§æµ‹è¯•\n\n")
            challenge3 = all_challenges['time_window']
            f.write("### æ ¸å¿ƒè´¨ç–‘\n")
            f.write("12ä¸ªæœˆçš„æ—¶é—´çª—å£æ˜¯å¦åˆé€‚ï¼Ÿä¸åŒæ—¶é—´çª—å£ä¼šå¦‚ä½•å½±å“æ´»è·ƒåº¦å’Œæ’åï¼Ÿ\n\n")

            f.write("### æµ‹è¯•æ–¹æ³•\n")
            f.write("æµ‹è¯•6/9/12/15/18ä¸ªæœˆçš„æ—¶é—´çª—å£ï¼Œæ¯”è¾ƒåˆ›ä½œè€…æ´»è·ƒåº¦å’Œåˆ†åŒºæ’åå˜åŒ–ã€‚\n\n")

            f.write("### æµ‹è¯•ç»“æœ\n")
            f.write("| æ—¶é—´çª—å£ | ä¸12ä¸ªæœˆçš„Top5é‡å åº¦ |\n")
            f.write("|----------|---------------------|\n")

            for window, stability in challenge3['time_stability'].items():
                f.write(f"| {window} | {stability:.1%} |\n")

            f.write(f"\n**ç»“è®º**: å¹³å‡ç¨³å®šæ€§ {challenge3['avg_time_stability']:.1%}ï¼Œ12ä¸ªæœˆçª—å£é€‰æ‹©è¾ƒä¸ºåˆç†ã€‚\n\n")

            # æŒ‘æˆ˜4ï¼šæƒé‡æ•æ„Ÿæ€§
            f.write("## ğŸ” æŒ‘æˆ˜4ï¼šæƒé‡å‚æ•°çš„æ•æ„Ÿæ€§åˆ†æ\n\n")
            challenge4 = all_challenges['weight_sensitivity']
            f.write("### æ ¸å¿ƒè´¨ç–‘\n")
            f.write("\"çˆ±çœ‹ç¨‹åº¦\"æŒ‡æ•°çš„æƒé‡è®¾ç½®ï¼ˆ0.5/0.3/0.2ï¼‰æ˜¯å¦åˆç†ï¼Ÿæƒé‡å˜åŒ–å¯¹ç»“æœå½±å“å¤šå¤§ï¼Ÿ\n\n")

            f.write("### æµ‹è¯•æ–¹æ³•\n")
            f.write("æµ‹è¯•5ç§æƒé‡ç»„åˆï¼ŒåŒ…æ‹¬æ’­æ”¾ä¸»å¯¼ã€äº’åŠ¨ä¸»å¯¼ã€ä¸Šæ¦œä¸»å¯¼å’Œå‡è¡¡æƒé‡ç­‰ã€‚\n\n")

            f.write("### æµ‹è¯•ç»“æœ\n")
            f.write("| æƒé‡æ–¹æ¡ˆ | Top5é‡å åº¦ |\n")
            f.write("|----------|------------|\n")

            for scenario, stability in challenge4['weight_stability'].items():
                f.write(f"| {scenario} | {stability:.1%} |\n")

            f.write(f"\n**ç»“è®º**: å¹³å‡ç¨³å®šæ€§ {challenge4['avg_weight_stability']:.1%}ï¼Œæƒé‡è®¾ç½®å¯¹ç»“æœå½±å“é€‚ä¸­ã€‚\n\n")

            # æŒ‘æˆ˜5ï¼šSDIå‡è®¾æ£€éªŒ
            f.write("## ğŸ” æŒ‘æˆ˜5ï¼šSDIè¯„åˆ†å‡è®¾çš„åˆç†æ€§æ£€éªŒ\n\n")
            challenge5 = all_challenges['sdi_assumption']
            f.write("### æ ¸å¿ƒè´¨ç–‘\n")
            f.write("SDIå››ä¸ªç»´åº¦çš„ç­‰æƒé‡å‡è®¾æ˜¯å¦åˆç†ï¼Ÿä¸åŒæƒé‡ä¸‹çš„åˆ†ç±»ç¨³å®šæ€§å¦‚ä½•ï¼Ÿ\n\n")

            f.write("### æµ‹è¯•æ–¹æ³•\n")
            f.write("æµ‹è¯•5ç§SDIæƒé‡ç»„åˆï¼Œè§‚å¯Ÿé«˜/ä¸­/ä½è„šæœ¬ä¾èµ–åˆ†ç±»çš„ç¨³å®šæ€§ã€‚\n\n")

            f.write("### æµ‹è¯•ç»“æœ\n")
            f.write("| æƒé‡æ–¹æ¡ˆ | åˆ†ç±»ä¸€è‡´æ€§ |\n")
            f.write("|----------|------------|\n")

            for scenario, stability in challenge5['category_stability'].items():
                f.write(f"| {scenario} | {stability:.1%} |\n")

            f.write(f"\n**ç»“è®º**: å¹³å‡ä¸€è‡´æ€§ {challenge5['avg_sdi_stability']:.1%}ï¼ŒSDIåˆ†ç±»ç›¸å¯¹ç¨³å®šã€‚\n\n")

            # ç»¼åˆè¯„ä¼°
            f.write("## ğŸ“Š ç»¼åˆç¨³å¥æ€§è¯„ä¼°\n\n")

            overall_stability = np.mean([
                avg_similarity,
                challenge2['avg_stability'],
                challenge3['avg_time_stability'],
                challenge4['avg_weight_stability'],
                challenge5['avg_sdi_stability']
            ])

            f.write(f"**æ•´ä½“ç¨³å¥æ€§è¯„åˆ†**: {overall_stability:.1%}\n\n")

            if overall_stability >= 0.8:
                f.write("âœ… **ç»“è®º**: åˆ†æç»“æœå…·æœ‰è¾ƒé«˜ç¨³å¥æ€§ï¼Œæ ¸å¿ƒå‘ç°å¯ä¿¡åº¦é«˜ã€‚\n\n")
            elif overall_stability >= 0.6:
                f.write("âš ï¸ **ç»“è®º**: åˆ†æç»“æœç¨³å¥æ€§ä¸­ç­‰ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯å…³é”®å‡è®¾ã€‚\n\n")
            else:
                f.write("âŒ **ç»“è®º**: åˆ†æç»“æœç¨³å¥æ€§è¾ƒä½ï¼Œéœ€è¦é‡æ–°å®¡è§†æ–¹æ³•å’Œå‡è®¾ã€‚\n\n")

            # æ”¹è¿›å»ºè®®
            f.write("## ğŸ’¡ æ”¹è¿›å»ºè®®\n\n")
            f.write("### æ–¹æ³•è®ºä¼˜åŒ–\n")
            f.write("1. **å¤šæºæ•°æ®éªŒè¯**: æ•´åˆé£ç“œã€ç«çƒ§äº‘ã€å¡æ€ç­‰å¤šä¸ªå¹³å°æ•°æ®\n")
            f.write("2. **åŠ¨æ€æƒé‡è°ƒæ•´**: æ ¹æ®åˆ†åŒºç‰¹å¾åŠ¨æ€è°ƒæ•´æŒ‡æ ‡æƒé‡\n")
            f.write("3. **ç½®ä¿¡åŒºé—´æŠ¥å‘Š**: ä¸ºå…³é”®æŒ‡æ ‡æä¾›ç½®ä¿¡åŒºé—´è€Œéç‚¹ä¼°è®¡\n")
            f.write("4. **åˆ†å±‚æŠ½æ ·**: æŒ‰åˆ›ä½œè€…è§„æ¨¡åˆ†å±‚ï¼Œå‡å°‘å¤´éƒ¨è´¦å·çš„å½±å“\n\n")

            f.write("### æ•°æ®è´¨é‡æå‡\n")
            f.write("1. **å®æ—¶æ•°æ®æ›´æ–°**: å»ºç«‹æœˆåº¦æ•°æ®æ›´æ–°æœºåˆ¶\n")
            f.write("2. **æ•°æ®è´¨é‡ç›‘æ§**: è®¾ç½®å¼‚å¸¸å€¼æ£€æµ‹å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥\n")
            f.write("3. **æ ‡å‡†åŒ–å¤„ç†**: å»ºç«‹æ•°æ®é¢„å¤„ç†çš„æ ‡å‡†åŒ–æµç¨‹\n")
            f.write("4. **å¤–éƒ¨éªŒè¯**: å¯»æ‰¾å®˜æ–¹æˆ–ç¬¬ä¸‰æ–¹æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯\n\n")

            f.write("### åˆ†ææ¡†æ¶å®Œå–„\n")
            f.write("1. **åœºæ™¯åˆ†æ**: ä¸ºä¸åŒåº”ç”¨åœºæ™¯å®šåˆ¶åŒ–åˆ†æå£å¾„\n")
            f.write("2. **è¶‹åŠ¿è·Ÿè¸ª**: å¢åŠ æ—¶é—´åºåˆ—åˆ†æï¼Œæ•æ‰åŠ¨æ€å˜åŒ–\n")
            f.write("3. **ç”¨æˆ·ç»†åˆ†**: æŒ‰ç²‰ä¸è§„æ¨¡ã€å†…å®¹ç±»å‹ç­‰ç»´åº¦ç»†åˆ†åˆ†æ\n")
            f.write("4. **é¢„æµ‹å»ºæ¨¡**: åŸºäºå†å²æ•°æ®å»ºç«‹å¢é•¿é¢„æµ‹æ¨¡å‹\n\n")

            # é£é™©æç¤º
            f.write("## âš ï¸ é£é™©æç¤ºä¸ä½¿ç”¨å»ºè®®\n\n")
            f.write("### ä¸»è¦é£é™©\n")
            f.write("1. **æ•°æ®æ—¶æ•ˆæ€§**: ç¤¾äº¤åª’ä½“æ•°æ®å˜åŒ–å¿«ï¼Œåˆ†æç»“æœæœ‰æ—¶æ•ˆæ€§\n")
            f.write("2. **å¹³å°æ”¿ç­–å˜åŒ–**: ç®—æ³•è°ƒæ•´å¯èƒ½å½±å“å†…å®¹åˆ†å‘å’Œç”¨æˆ·è¡Œä¸º\n")
            f.write("3. **æ ·æœ¬ä»£è¡¨æ€§**: ç¬¬ä¸‰æ–¹å¹³å°æ•°æ®å¯èƒ½æ— æ³•å®Œå…¨ä»£è¡¨å…¨å¹³å°æƒ…å†µ\n")
            f.write("4. **å› æœæ¨æ–­é™åˆ¶**: ç›¸å…³æ€§åˆ†ææ— æ³•ç¡®å®šå› æœå…³ç³»\n\n")

            f.write("### ä½¿ç”¨å»ºè®®\n")
            f.write("1. **ç»“åˆå®šæ€§åˆ†æ**: é‡åŒ–åˆ†æéœ€è¦ç»“åˆè¡Œä¸šä¸“å®¶çš„å®šæ€§åˆ¤æ–­\n")
            f.write("2. **åˆ†é˜¶æ®µéªŒè¯**: å°è§„æ¨¡è¯•ç‚¹éªŒè¯åå†å¤§è§„æ¨¡åº”ç”¨\n")
            f.write("3. **æŒç»­ç›‘æ§**: å»ºç«‹ç»“æœè·Ÿè¸ªæœºåˆ¶ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥\n")
            f.write("4. **å¤šç»´åº¦å†³ç­–**: å°†åˆ†æç»“æœä½œä¸ºå†³ç­–å‚è€ƒä¹‹ä¸€ï¼Œè€Œéå”¯ä¸€ä¾æ®\n\n")

            f.write("---\n")
            f.write("**æŠ¥å‘Šè¯´æ˜**: æœ¬æŒ‘æˆ˜åˆ†æåŸºäºå½“å‰æ•°æ®å’Œå‡è®¾ï¼Œéšç€æ•°æ®è´¨é‡å’Œæ–¹æ³•æ”¹è¿›ï¼Œç»“è®ºå¯èƒ½éœ€è¦æ›´æ–°ã€‚\n")

        logger.info(f"ğŸ“ æŒ‘æˆ˜åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

    def run(self):
        """è¿è¡Œå®Œæ•´çš„æŒ‘æˆ˜åˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹æŒ‘æˆ˜åˆ†æä¸æ•æ„Ÿæ€§æµ‹è¯•...")

        # åŠ è½½æ•°æ®
        popularity_df, creators_df, videos_df, sdi_df = self.load_base_data()

        # æ‰§è¡Œ5ä¸ªæŒ‘æˆ˜åˆ†æ
        challenges = {}

        challenges['fans_duplication'] = self.challenge_1_fans_duplication(creators_df)
        challenges['data_source_bias'] = self.challenge_2_data_source_bias(popularity_df, videos_df)
        challenges['time_window'] = self.challenge_3_time_window_sensitivity(creators_df, videos_df)
        challenges['weight_sensitivity'] = self.challenge_4_weight_sensitivity(popularity_df)
        challenges['sdi_assumption'] = self.challenge_5_sdi_assumption_test(sdi_df)

        # ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_challenge_report(challenges)

        logger.info("âœ… æŒ‘æˆ˜åˆ†æå®Œæˆ!")

        return {
            'challenges': challenges,
            'report_file': report_file
        }

if __name__ == "__main__":
    analyzer = ChallengeAnalyzer("../")
    results = analyzer.run()

    print("\nğŸ¯ æŒ‘æˆ˜åˆ†æå®Œæˆ!")
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {results['report_file']}")
    print(f"ğŸ” å…±å®Œæˆ {len(results['challenges'])} é¡¹æŒ‘æˆ˜åˆ†æ")